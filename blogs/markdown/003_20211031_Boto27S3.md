---
title: 'Boto 2.7 S3 Upload'
date: 'October 31 2021'
excerpt: 'Boto Python 2.7, Upload to S3 Bucket'
cover_image: 'images/posts/003_Boto27.png'
tag: 'dev:python27:boto'
---

# Upload to an S3 Bucket with Boto for Python 2.7

Python 2.7 is at end of life, however some applications are still utilizing Python 2.7.  Thusly I had a need to write a tool in order to upload to S3 bucket in Python 2.7.

## S3 Buckets

Object storage for AWS, data retention policies, access control, event notifications and high durability make in an excellent option for my data exchange. 

You'll need to make a bucket with it's OWN BUCKET POLICY, as well as IAM user with limited permissions, LEAST PERMISSIONS. 

**Bucket Policy**

Need to allow the user on all resources on the bucket as well as the bucket itself.

```bash
    "Statement": [
        {
            "Sid": "Stmt1634652086039",
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::<ARN-of-the-user-you-created>"
            },
            "Action": "s3:*",
            "Resource": [
                "arn:aws:s3:::youbucketname/*",
                "arn:aws:s3:::yourbucketname"
            ]
        }
    ]
```

You'll need an ACCESS_KEY and SECRET_KEY for the programmatic access user you created, with the least possible permissions for this action, S3GetObject & S3PutObject

**IAM Permissions**

```bash
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": [
                "s3:PutObject",
                "s3:GetObject"
            ],
            "Resource": "arn:aws:s3:::yourbucketname"
        }
    ]
```

## Boto in Python

[Boto Docs](https://boto.readthedocs.io/en/glacier/s3_tut.html)

Make a variable for your ACCESS_KEY and SECRET_KEY

```python
AWS_ACCESS_KEY_ID = '<your_access_key>'
AWS_SECRET_ACCESS_KEY = '<your_secret_key>'
bucket_name = '<your_bucket_name>'
```
Import boto S3 connection and create bucket connection

```python
from boto.s3.connection import S3Connection
conn = S3Connection(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)
bucket = conn.get_bucket(Bucketname)
```

Import operating system, get basename of file and file size. 

```python
import os
source_path = r"path\to\your\source\file"

#Get file basename
upload_name = os.path.basename(source_path)

#Get File Size
source_size = os.stat(source_path).st_size
```

Initialize a multipart upload on the S3 bucket.  If you need to upload to specific prefix, specify the prefix in the upload. 

```python
mp = bucket.initiate_multipart_upload('<optional-file-prefix>/' + upload_name)
```

We need to create file chunks and then iterate throught those chunks to upload.

```python
import math
from filechunkio import FileChunkIO

#Create chunk
chunk_size = 52428800
chunk_count = int(math.ceil(source_size / float(chunk_size)))

for i in range(chunk_count):
   offset = chunk_size * i
   byts = min(chunk_size, source_size - offset)
   with FileChunkIO(source_path, 'r', offset=offset, bytes=byts) as fp:
       mp.upload_part_from_file(fp, part_num=i + 1) 
```

When we've interated through the chunks we need to mark the upload complete.

```python
mp.complete_upload()
```

